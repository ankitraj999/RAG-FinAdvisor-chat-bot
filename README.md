Demo link: https://www.youtube.com/watch?v=9rlxBpXdbOQ

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```
# RAG Chatbot with Llama 3 and Pinecone

This project implements a Retrieval-Augmented Generation (RAG) chatbot using Llama 3 as the Language Model (LLM) and Pinecone as the vector database. The frontend is built with Next.js for a responsive and efficient user interface.

## Features

- Conversational AI powered by Llama 3
- Efficient information retrieval using Pinecone vector database
- Context-aware responses based on conversation history
- Seamless integration with Next.js frontend
- API-based architecture for flexible deployment

## Tech Stack

- **Language Model**: Llama 3
- **Vector Database**: Pinecone
- **Backend**: Python with Flask
- **Frontend**: Next.js
- **Embedding Model**: Sentence Transformers (all-MiniLM-L6-v2)


